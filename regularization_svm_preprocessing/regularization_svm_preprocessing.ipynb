{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regularization-svm-preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMXFNZEowfck4xzupxP5+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muneeb706/machine-learning/blob/main/regularization_svm_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM79JB8A-FA6",
        "outputId": "3148c107-4f09-4705-bbf3-a5cedc4ae4f3"
      },
      "source": [
        " !pip install -U liblinear-official"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting liblinear-official\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/91/859f0db15b868aaa671b84fbbff3f787f0655a13d334fd29b56e1c7dc938/liblinear-official-2.43.0.tar.gz (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from liblinear-official) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->liblinear-official) (1.19.5)\n",
            "Building wheels for collected packages: liblinear-official\n",
            "  Building wheel for liblinear-official (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liblinear-official: filename=liblinear_official-2.43.0-cp37-cp37m-linux_x86_64.whl size=124242 sha256=fe1bfca9f2a8b9e6c2575cd927a50eac15f278985e197353c023ec0860ef2dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/66/25/ae78a3b8319bbc49d770ada1ba02fc1274b93dce2e5b1c8b61\n",
            "Successfully built liblinear-official\n",
            "Installing collected packages: liblinear-official\n",
            "Successfully installed liblinear-official-2.43.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUo7iOeA5Lae",
        "outputId": "ed862a59-4c66-4ca4-c763-e52b45fbe596"
      },
      "source": [
        " !pip install -U libsvm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting libsvm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/11/c7700d0cd3a21eef2d7d996256277fc640ccd4f84717c10228cb6c1567dc/libsvm-3.23.0.4.tar.gz (170kB)\n",
            "\r\u001b[K     |██                              | 10kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 163kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: libsvm\n",
            "  Building wheel for libsvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp37-cp37m-linux_x86_64.whl size=233325 sha256=feae055b4ad852b200c91bb5c4305f4b61af1b485d92e201a4484f7fcebccee4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/9e/b5/dbb033107407eec2f52b8cd24cf024a4b9ec8b62ea5aee995a\n",
            "Successfully built libsvm\n",
            "Installing collected packages: libsvm\n",
            "Successfully installed libsvm-3.23.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgp4v4mzsw3V"
      },
      "source": [
        "I will use Python Interface for following questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piPAqBqS-9-E"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from liblinear.liblinearutil import *"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6NFBHfpl0U2"
      },
      "source": [
        "## Problem 1: Regularized Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh96eB8Bl6u-"
      },
      "source": [
        "### Part - 2: Train and test a regularized logistic regression model one two data sets, namely the breast cancer and sonar data sets which are available here https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#breast-cancer and here https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#sonar. We will use the scaled version for our experiment.  A copy of them is also enclosed in this homework. Use the provided train-ing/testing splitting. In particular, the file \\xxx-scale-test-indices.txt\" contains the indices of examples in the original file for training, and \\xxx-scale-test-indices.txt\" contains the indices of examples in the original file for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MvaHboMxTwx"
      },
      "source": [
        "### 1) Working with Breast Cancer Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6wv3sCimjG5"
      },
      "source": [
        "Reading data in LIBSVM format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZf1Ao9V_Gbl"
      },
      "source": [
        " y_breast, x_breast = svm_read_problem('./sample_data/breast-cancer_scale.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8OYw2hmvuc"
      },
      "source": [
        "Reading training and testing indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VASqH2LhG0WX"
      },
      "source": [
        "with open('./sample_data/breast-cancer-scale-train-indices.txt') as f:    \n",
        "    breast_train_indexes = [int(line) for line in f]\n",
        "with open('./sample_data/breast-cancer-scale-test-indices.txt') as f:    \n",
        "    breast_test_indexes = [int(line) for line in f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3tNEStlm0r9"
      },
      "source": [
        "Splitting training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op-vvkwjJyoc"
      },
      "source": [
        "x_breast_train = [x_breast[i - 1] for i in breast_train_indexes]\n",
        "y_breast_train = [y_breast[i - 1] for i in breast_train_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqOAcjKjNjkY"
      },
      "source": [
        "x_breast_test = [x_breast[i - 1] for i in breast_test_indexes]\n",
        "y_breast_test = [y_breast[i - 1] for i in breast_test_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw2KBipRm7lB"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EsgGS3oT2UW",
        "outputId": "5c849ed6-2650-4569-e7cd-ccb72af2609c"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -v 5 -c ' + C\n",
        "  CV_ACC = train(y_breast_train, x_breast_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = predict(y_breast_train, x_breast_train, train(y_breast_train, x_breast_train, '-s 0 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 95.4%\n",
            "Validation Error: 4.6000000000000085\n",
            "Accuracy = 95.8% (479/500) (classification)\n",
            "Training Error: 4.200000000000003\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 97%\n",
            "Validation Error: 3.0\n",
            "Accuracy = 97.2% (486/500) (classification)\n",
            "Training Error: 2.799999999999997\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 97%\n",
            "Validation Error: 3.0\n",
            "Accuracy = 97.6% (488/500) (classification)\n",
            "Training Error: 2.4000000000000057\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 97.4%\n",
            "Validation Error: 2.6000000000000085\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 96.8%\n",
            "Validation Error: 3.200000000000003\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "Best C = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEEjBaAprD64"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5jQESzXak-L",
        "outputId": "fe26f21a-f1f2-44df-cf72-f18b07bda41e"
      },
      "source": [
        "# training model\n",
        "prob  = problem(y_breast_train, x_breast_train)\n",
        "param = parameter('-s 0 -c ' + min_val_error_C)\n",
        "m = train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"Logistic Regression (Breast Cancer): \")\n",
        "p_label, p_acc, p_val = predict(y_breast_test, x_breast_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_breast_test, p_label)\n",
        "lr_breast_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(lr_breast_error_rate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression (Breast Cancer): \n",
            "Accuracy = 96.1749% (176/183) (classification)\n",
            "Test Error Rate = 3.825136612021865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9BABCBxxbpx"
      },
      "source": [
        "### 2) Working with Sonar Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cFCJenuxl6R"
      },
      "source": [
        "Reading data in LIBSVM format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GFJNHK3xhkK"
      },
      "source": [
        " y_sonar, x_sonar = svm_read_problem('./sample_data/sonar_scale.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ4pQzPExsPJ"
      },
      "source": [
        "Reading training and testing indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTfOToF0xzOB"
      },
      "source": [
        "with open('./sample_data/sonar-scale-train-indices.txt') as f:    \n",
        "    sonar_train_indexes = [int(line) for line in f]\n",
        "with open('./sample_data/sonar-scale-test-indices.txt') as f:    \n",
        "    sonar_test_indexes = [int(line) for line in f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace9dYjXx_eC"
      },
      "source": [
        "Splitting training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP9lw8zwyAH6"
      },
      "source": [
        "x_sonar_train = [x_sonar[i - 1] for i in sonar_train_indexes]\n",
        "y_sonar_train = [y_sonar[i - 1] for i in sonar_train_indexes]\n",
        "x_sonar_test = [x_sonar[i - 1] for i in sonar_test_indexes]\n",
        "y_sonar_test = [y_sonar[i - 1] for i in sonar_test_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRNv4jXpyLua"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbxa8C8OyMTV",
        "outputId": "3d619624-df87-48ba-e275-c974b42e8c4b"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -v 5 -c ' + C\n",
        "  CV_ACC = train(y_sonar_train, x_sonar_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = predict(y_sonar_train, x_sonar_train, train(y_sonar_train, x_sonar_train, '-s 0 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 76%\n",
            "Validation Error: 24.0\n",
            "Accuracy = 81.3333% (122/150) (classification)\n",
            "Training Error: 18.66666666666667\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 72.6667%\n",
            "Validation Error: 27.33333333333333\n",
            "Accuracy = 89.3333% (134/150) (classification)\n",
            "Training Error: 10.666666666666671\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 76.6667%\n",
            "Validation Error: 23.33333333333333\n",
            "Accuracy = 94.6667% (142/150) (classification)\n",
            "Training Error: 5.333333333333329\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 74.6667%\n",
            "Validation Error: 25.33333333333333\n",
            "Accuracy = 98% (147/150) (classification)\n",
            "Training Error: 2.0\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 72.6667%\n",
            "Validation Error: 27.33333333333333\n",
            "Accuracy = 100% (150/150) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "Best C = 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgsfjBFIyUm6"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3i6ng77yVRR",
        "outputId": "dd07ba27-eddb-40ba-acd0-73cc8b07e3b0"
      },
      "source": [
        "# training model\n",
        "prob  = problem(y_sonar_train, x_sonar_train)\n",
        "param = parameter('-s 0 -c ' + min_val_error_C)\n",
        "m = train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"Logistic Regression (Sonar Dataset): \")\n",
        "p_label, p_acc, p_val = predict(y_sonar_test, x_sonar_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_sonar_test, p_label)\n",
        "lr_sonar_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(lr_sonar_error_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression (Sonar Dataset): \n",
            "Accuracy = 72.4138% (42/58) (classification)\n",
            "Test Error Rate = 27.58620689655173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZLVQRPIs9HD"
      },
      "source": [
        "## Problem 2: Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKv1GmStFPS"
      },
      "source": [
        "### Part - 1: Repeating the same experiments as in Problem 1 by using linear SVM. To train a linear SVM by liblinear you can use the option \"-s 3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TII_0OfazCTM"
      },
      "source": [
        "### 1) Working with Breast Cancer Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBPbX_Uznyq"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzPPuJBaznyr",
        "outputId": "2b1f0d7d-30ce-4a4b-c6cf-2879a8d5b191"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 3 -v 5 -c ' + C\n",
        "  CV_ACC = train(y_breast_train, x_breast_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = predict(y_breast_train, x_breast_train, train(y_breast_train, x_breast_train, '-s 3 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 96.8%\n",
            "Validation Error: 3.200000000000003\n",
            "Accuracy = 97% (485/500) (classification)\n",
            "Training Error: 3.0\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 97%\n",
            "Validation Error: 3.0\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 97.2%\n",
            "Validation Error: 2.799999999999997\n",
            "Accuracy = 98% (490/500) (classification)\n",
            "Training Error: 2.0\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 93.6%\n",
            "Validation Error: 6.3999999999999915\n",
            "Accuracy = 97.4% (487/500) (classification)\n",
            "Training Error: 2.6000000000000085\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 91.6%\n",
            "Validation Error: 8.399999999999991\n",
            "Accuracy = 96.8% (484/500) (classification)\n",
            "Training Error: 3.200000000000003\n",
            "\n",
            "Best C = 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAV3DqOr0Glk"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XerfNubg0Gll",
        "outputId": "0859141e-99ee-4ee2-f913-4fe5ab1de95a"
      },
      "source": [
        "# training model\n",
        "prob  = problem(y_breast_train, x_breast_train)\n",
        "param = parameter('-s 3 -c ' + min_val_error_C)\n",
        "m = train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"Linear SVM (Breast Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = predict(y_breast_test, x_breast_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_breast_test, p_label)\n",
        "lsvm_breast_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(lsvm_breast_error_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear SVM (Breast Dataset): \n",
            "Accuracy = 96.7213% (177/183) (classification)\n",
            "Test Error Rate = 3.278688524590166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZfFrxWE0Vmm"
      },
      "source": [
        "### 2) Working with Sonar Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1zmN1wg0Vmo",
        "outputId": "3f5bf299-443a-449c-c80f-e726db8834ae"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 3 -v 5 -c ' + C\n",
        "  CV_ACC = train(y_sonar_train, x_sonar_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = predict(y_sonar_train, x_sonar_train, train(y_sonar_train, x_sonar_train, '-s 3 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 70%\n",
            "Validation Error: 30.0\n",
            "Accuracy = 84% (126/150) (classification)\n",
            "Training Error: 16.0\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 69.3333%\n",
            "Validation Error: 30.666666666666657\n",
            "Accuracy = 92.6667% (139/150) (classification)\n",
            "Training Error: 7.333333333333343\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 68.6667%\n",
            "Validation Error: 31.33333333333333\n",
            "Accuracy = 96% (144/150) (classification)\n",
            "Training Error: 4.0\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 71.3333%\n",
            "Validation Error: 28.666666666666657\n",
            "Accuracy = 96% (144/150) (classification)\n",
            "Training Error: 4.0\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 67.3333%\n",
            "Validation Error: 32.66666666666667\n",
            "Accuracy = 95.3333% (143/150) (classification)\n",
            "Training Error: 4.666666666666657\n",
            "\n",
            "Best C = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp9mDs2k0Vmo"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3kpqTDE0Vmp",
        "outputId": "b00cbb11-da40-4dac-9570-447c55d9afb0"
      },
      "source": [
        "# training model\n",
        "prob  = problem(y_sonar_train, x_sonar_train)\n",
        "param = parameter('-s 3 -c ' + min_val_error_C)\n",
        "m = train(prob, param)\n",
        "\n",
        "# predicting\n",
        "print(\"Linear SVM (Sonar Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = predict(y_sonar_test, x_sonar_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_sonar_test, p_label)\n",
        "lsvm_sonar_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(lsvm_sonar_error_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear SVM (Sonar Dataset): \n",
            "Accuracy = 70.6897% (41/58) (classification)\n",
            "Test Error Rate = 29.310344827586206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rfyHEv60v8v"
      },
      "source": [
        "### Part - 2: Repeat the same experiments as in Problem 1 by using kernel SVM. To train a kernel SVM by libsvm you can use the option \"-s 0\". Use the optional \"-t \" to choose different types of kernels. Try polynomial kernel and RBF kernel with default values of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqLTLnsu5eAV"
      },
      "source": [
        "from libsvm.svmutil import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSMbkwU04PEQ"
      },
      "source": [
        "### 1) Working with Breast Cancer Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEiJyihM4PEY"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbFQ5y3u_Zku"
      },
      "source": [
        "#### Polynomial Kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rAXV_7Y4PEY",
        "outputId": "9a52c46f-71f1-4e39-e6c5-d1cf8ee00407"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -t 1 -v 5 -c ' + C\n",
        "  CV_ACC = svm_train(y_breast_train, x_breast_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = svm_predict(y_breast_train, x_breast_train, svm_train(y_breast_train, x_breast_train, '-s 0 -t 1 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 97.4%\n",
            "Validation Error: 2.6000000000000085\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 97.6%\n",
            "Validation Error: 2.4000000000000057\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 97.2%\n",
            "Validation Error: 2.799999999999997\n",
            "Accuracy = 98.6% (493/500) (classification)\n",
            "Training Error: 1.4000000000000057\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 95.6%\n",
            "Validation Error: 4.400000000000006\n",
            "Accuracy = 99.2% (496/500) (classification)\n",
            "Training Error: 0.7999999999999972\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 95.2%\n",
            "Validation Error: 4.800000000000011\n",
            "Accuracy = 100% (500/500) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "Best C = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIaIhF84PEZ"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpamrv9Q4PEa",
        "outputId": "3d8be84e-0151-4b71-c28e-e5a0ef021339"
      },
      "source": [
        "# training model\n",
        "prob  = svm_problem(y_breast_train, x_breast_train)\n",
        "param = svm_parameter('-s 0 -t 1 -c ' + min_val_error_C)\n",
        "m = svm_train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"Polynomial Kernel SVM (Breast Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = svm_predict(y_breast_test, x_breast_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_breast_test, p_label)\n",
        "pksvm_breast_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(pksvm_breast_error_rate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Polynomial Kernel SVM (Breast Dataset): \n",
            "Accuracy = 96.7213% (177/183) (classification)\n",
            "Test Error Rate = 3.278688524590166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq5MSkccBngX"
      },
      "source": [
        "#### RBF Kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd0xyzr1Bngi",
        "outputId": "15a1551a-ae67-4f03-9aec-5314f09a103f"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -t 2 -v 5 -c ' + C\n",
        "  CV_ACC = svm_train(y_breast_train, x_breast_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = svm_predict(y_breast_train, x_breast_train, svm_train(y_breast_train, x_breast_train, '-s 0 -t 2 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 97%\n",
            "Validation Error: 3.0\n",
            "Accuracy = 97.4% (487/500) (classification)\n",
            "Training Error: 2.6000000000000085\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 97.8%\n",
            "Validation Error: 2.200000000000003\n",
            "Accuracy = 97.8% (489/500) (classification)\n",
            "Training Error: 2.200000000000003\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 97.2%\n",
            "Validation Error: 2.799999999999997\n",
            "Accuracy = 98.4% (492/500) (classification)\n",
            "Training Error: 1.5999999999999943\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 96.2%\n",
            "Validation Error: 3.799999999999997\n",
            "Accuracy = 99.6% (498/500) (classification)\n",
            "Training Error: 0.4000000000000057\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 95.2%\n",
            "Validation Error: 4.800000000000011\n",
            "Accuracy = 100% (500/500) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "Best C = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nClHbsOSBngi"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjMJeC92Bngj",
        "outputId": "72ae0ba0-c98c-43c9-ee03-669f18b45f02"
      },
      "source": [
        "# training model\n",
        "prob  = svm_problem(y_breast_train, x_breast_train)\n",
        "param = svm_parameter('-s 0 -t 2 -c ' + min_val_error_C)\n",
        "m = svm_train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"RBF Kernel SVM (Breast Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = svm_predict(y_breast_test, x_breast_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_breast_test, p_label)\n",
        "rbfksvm_breast_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(rbfksvm_breast_error_rate))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RBF Kernel SVM (Breast Dataset): \n",
            "Accuracy = 96.1749% (176/183) (classification)\n",
            "Test Error Rate = 3.825136612021865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8m_0Mtg4PEa"
      },
      "source": [
        "### 2) Working with Sonar Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBauuzCLDTBP"
      },
      "source": [
        "#### Polynomial Kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGOQqnvQ4PEb"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PMfBHXT4PEb",
        "outputId": "6f257311-8667-4f9d-a42d-15e39cce7ebf"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -t 1 -v 5 -c ' + C\n",
        "  CV_ACC = svm_train(y_sonar_train, x_sonar_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = svm_predict(y_sonar_train, x_sonar_train, svm_train(y_sonar_train, x_sonar_train, '-s 0 -t 1 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 56%\n",
            "Validation Error: 43.99999999999999\n",
            "Accuracy = 56% (84/150) (classification)\n",
            "Training Error: 43.99999999999999\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 56%\n",
            "Validation Error: 43.99999999999999\n",
            "Accuracy = 60% (90/150) (classification)\n",
            "Training Error: 40.0\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 78.6667%\n",
            "Validation Error: 21.333333333333343\n",
            "Accuracy = 84% (126/150) (classification)\n",
            "Training Error: 16.0\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 84.6667%\n",
            "Validation Error: 15.333333333333329\n",
            "Accuracy = 100% (150/150) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 86%\n",
            "Validation Error: 14.0\n",
            "Accuracy = 100% (150/150) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "Best C = 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjOZoob4PEc"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTwDXWNw4PEc",
        "outputId": "ee351ffd-5370-4656-ca5c-0715c91dff71"
      },
      "source": [
        "# training model\n",
        "prob  = svm_problem(y_sonar_train, x_sonar_train)\n",
        "param = svm_parameter('-s 0 -t 1 -c ' + min_val_error_C)\n",
        "m = svm_train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"Polynomial Kernel SVM (Sonar Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = svm_predict(y_sonar_test, x_sonar_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_sonar_test, p_label)\n",
        "pk_sonar_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(pk_sonar_error_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Polynomial Kernel SVM (Sonar Dataset): \n",
            "Accuracy = 86.2069% (50/58) (classification)\n",
            "Test Error Rate = 13.793103448275872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpZQ0deEHM8"
      },
      "source": [
        "#### RBF Kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiTRGJ37EO3l"
      },
      "source": [
        "Using the 5-fold cross validation method to decide the best value of the parameter C. The candidate values for C are 0.1, 1, 10, 100, 1000. For each C, report the training error and\n",
        "validation error. Choosing the best C that yields the lowest validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2JzDi5KEJgN",
        "outputId": "a0844be4-a7d0-4e4b-db7f-7d0a8e4fcd56"
      },
      "source": [
        "candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "min_val_error = 100\n",
        "min_val_error_C = candidate_C[0]\n",
        "for C in candidate_C:\n",
        "  print(\"For C = \" + C)\n",
        "  parameters = '-s 0 -t 2 -v 5 -c ' + C\n",
        "  CV_ACC = svm_train(y_sonar_train, x_sonar_train, parameters)\n",
        "  val_error = 100 - CV_ACC\n",
        "  print(\"Validation Error: \" + str(val_error))\n",
        "  p_label, p_acc, p_val = svm_predict(y_sonar_train, x_sonar_train, svm_train(y_sonar_train, x_sonar_train, '-s 0 -t 2 -c ' + C))\n",
        "  print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    min_val_error_C = C\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "print(\"Best C = \" + min_val_error_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For C = 0.1\n",
            "Cross Validation Accuracy = 56%\n",
            "Validation Error: 43.99999999999999\n",
            "Accuracy = 56% (84/150) (classification)\n",
            "Training Error: 43.99999999999999\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 74.6667%\n",
            "Validation Error: 25.33333333333333\n",
            "Accuracy = 85.3333% (128/150) (classification)\n",
            "Training Error: 14.666666666666657\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 72.6667%\n",
            "Validation Error: 27.33333333333333\n",
            "Accuracy = 97.3333% (146/150) (classification)\n",
            "Training Error: 2.666666666666657\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 82.6667%\n",
            "Validation Error: 17.33333333333333\n",
            "Accuracy = 100% (150/150) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 81.3333%\n",
            "Validation Error: 18.66666666666667\n",
            "Accuracy = 100% (150/150) (classification)\n",
            "Training Error: 0.0\n",
            "\n",
            "Best C = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh09DpX5ERET"
      },
      "source": [
        "Using the selected best C value to train a logistic regression model on the whole training data and evaluating and reporting its performance (by error rate) on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwde7XEjESOv",
        "outputId": "25bf0610-577a-49a6-d1a4-7d94e5f56041"
      },
      "source": [
        "# training model\n",
        "prob  = svm_problem(y_sonar_train, x_sonar_train)\n",
        "param = svm_parameter('-s 0 -t 2 -c ' + min_val_error_C)\n",
        "m = svm_train(prob, param)\n",
        "\n",
        "# predicting\n",
        "\n",
        "print(\"RBF Kernel SVM (Sonar Dataset): \")\n",
        "\n",
        "p_label, p_acc, p_val = svm_predict(y_sonar_test, x_sonar_test, m)\n",
        "ACC, MSE, SCC = evaluations(y_sonar_test, p_label)\n",
        "rbfksvm_sonar_error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(pk_sonar_error_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RBF Kernel SVM (Sonar Dataset): \n",
            "Accuracy = 86.2069% (50/58) (classification)\n",
            "Test Error Rate = 13.793103448275872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrf1uIMhFdQx"
      },
      "source": [
        "### Part - 3: Compare the test error given by Logistic Regression, Linear SVM and Kernel SVMs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0e167G7FhtM",
        "outputId": "7290335a-ad92-4830-bd5a-ec27accea1a5"
      },
      "source": [
        "print(\"--- Breast Dataset Error Rates ---\")\n",
        "print(\"Logistic Regression: \" + str(lr_breast_error_rate))\n",
        "print(\"Linear SVM: \" + str(lsvm_breast_error_rate))\n",
        "print(\"Polynomial Kernel SVM: \" + str(pksvm_breast_error_rate))\n",
        "print(\"RBF Kernel SVM: \" + str(rbfksvm_breast_error_rate))\n",
        "print(\"\")\n",
        "print(\"--- Sonar Dataset Error Rates ---\")\n",
        "print(\"Logistic Regression: \" + str(lr_sonar_error_rate))\n",
        "print(\"Linear SVM: \" + str(lsvm_sonar_error_rate))\n",
        "print(\"Polynomial Kernel SVM: \" + str(pk_sonar_error_rate))\n",
        "print(\"RBF Kernel SVM: \" + str(rbfksvm_sonar_error_rate))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Breast Dataset Error Rates ---\n",
            "Logistic Regression: 3.825136612021865\n",
            "Linear SVM: 3.278688524590166\n",
            "Polynomial Kernel SVM: 3.278688524590166\n",
            "RBF Kernel SVM: 3.825136612021865\n",
            "\n",
            "--- Sonar Dataset Error Rates ---\n",
            "Logistic Regression: 27.58620689655173\n",
            "Linear SVM: 29.310344827586206\n",
            "Polynomial Kernel SVM: 13.793103448275872\n",
            "RBF Kernel SVM: 13.793103448275872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV1COu7GG4gY"
      },
      "source": [
        "In case of Breast Dataset, the error rates reported by all of the algorithms are similar. The difference between the error rates is not significant enough. This results suggest that data points in Breast Dataset are linearly separable. However, in case of Sonar Dataset, the performance of non-lienar SVMs are way better than the performance of Linear algorithms (Linear SVM and Logistic Regression). The error rates reported by non-linear SVMs are less than half of the error rates reported by linear algorithms. This shows the data points in Sonar Dataset are not linearly separable. Also, the error rates reported by Polynomial and Kernel SVM are very similar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3hoZxHNJxNV"
      },
      "source": [
        "## Problem 3: Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXJohRoq_XZM"
      },
      "source": [
        "### Part - 1: data preprocessing, training, and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgiVrqz9_z0b"
      },
      "source": [
        "Reading data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "IK9I217_pcLr",
        "outputId": "543ebed1-35e5-46c3-b031-9762b6b85251"
      },
      "source": [
        "covtype_data = pd.read_csv('./sample_data/covtype.data', sep=\",\", header=None)\n",
        "covtype_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>212</td>\n",
              "      <td>-6</td>\n",
              "      <td>390</td>\n",
              "      <td>220</td>\n",
              "      <td>235</td>\n",
              "      <td>151</td>\n",
              "      <td>6225</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1   2    3    4     5    6    7   ...  47  48  49  50  51  52  53  54\n",
              "0  2596   51   3  258    0   510  221  232  ...   0   0   0   0   0   0   0   5\n",
              "1  2590   56   2  212   -6   390  220  235  ...   0   0   0   0   0   0   0   5\n",
              "2  2804  139   9  268   65  3180  234  238  ...   0   0   0   0   0   0   0   2\n",
              "3  2785  155  18  242  118  3090  238  238  ...   0   0   0   0   0   0   0   2\n",
              "4  2595   45   2  153   -1   391  220  234  ...   0   0   0   0   0   0   0   5\n",
              "\n",
              "[5 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk5DpMdl_1qV"
      },
      "source": [
        "Separting target feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onEYLGF5_8nL"
      },
      "source": [
        "covtype_X, covtype_Y = covtype_data.iloc[:, 0:54], covtype_data.iloc[:, 54]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYCAtoltFHaI"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFBKvWE4FP02"
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUgpG7v2FIQu"
      },
      "source": [
        "##### 1) Rescaling (min-max)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "pZCjAP5XG0U3",
        "outputId": "88912a02-c757-4488-b6ad-328049054152"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "covtype_X_rescaled = pd.DataFrame(min_max_scaler.fit_transform(covtype_X))\n",
        "covtype_X_rescaled.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.368684</td>\n",
              "      <td>0.141667</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.184681</td>\n",
              "      <td>0.223514</td>\n",
              "      <td>0.071659</td>\n",
              "      <td>0.870079</td>\n",
              "      <td>0.913386</td>\n",
              "      <td>0.582677</td>\n",
              "      <td>0.875366</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.365683</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.151754</td>\n",
              "      <td>0.215762</td>\n",
              "      <td>0.054798</td>\n",
              "      <td>0.866142</td>\n",
              "      <td>0.925197</td>\n",
              "      <td>0.594488</td>\n",
              "      <td>0.867838</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.472736</td>\n",
              "      <td>0.386111</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.191840</td>\n",
              "      <td>0.307494</td>\n",
              "      <td>0.446817</td>\n",
              "      <td>0.921260</td>\n",
              "      <td>0.937008</td>\n",
              "      <td>0.531496</td>\n",
              "      <td>0.853339</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.463232</td>\n",
              "      <td>0.430556</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.173228</td>\n",
              "      <td>0.375969</td>\n",
              "      <td>0.434172</td>\n",
              "      <td>0.937008</td>\n",
              "      <td>0.937008</td>\n",
              "      <td>0.480315</td>\n",
              "      <td>0.865886</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.368184</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.109520</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.054939</td>\n",
              "      <td>0.866142</td>\n",
              "      <td>0.921260</td>\n",
              "      <td>0.590551</td>\n",
              "      <td>0.860449</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4   ...   49   50   51   52   53\n",
              "0  0.368684  0.141667  0.045455  0.184681  0.223514  ...  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.365683  0.155556  0.030303  0.151754  0.215762  ...  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.472736  0.386111  0.136364  0.191840  0.307494  ...  0.0  0.0  0.0  0.0  0.0\n",
              "3  0.463232  0.430556  0.272727  0.173228  0.375969  ...  0.0  0.0  0.0  0.0  0.0\n",
              "4  0.368184  0.125000  0.030303  0.109520  0.222222  ...  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chT5-KBqHZxo"
      },
      "source": [
        "##### 2) Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Q6kHIr7iH4Ky",
        "outputId": "24f17b04-edc5-4195-db87-89bb9fe6395d"
      },
      "source": [
        "scaler = preprocessing.StandardScaler().fit(covtype_X)\n",
        "covtype_X_standardized = pd.DataFrame(scaler.transform(covtype_X))\n",
        "covtype_X_standardized.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.297805</td>\n",
              "      <td>-0.935157</td>\n",
              "      <td>-1.482820</td>\n",
              "      <td>-0.053767</td>\n",
              "      <td>-0.796273</td>\n",
              "      <td>-1.180146</td>\n",
              "      <td>0.330743</td>\n",
              "      <td>0.439143</td>\n",
              "      <td>0.142960</td>\n",
              "      <td>3.246283</td>\n",
              "      <td>1.10808</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879364</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.091491</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076972</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010336</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.21498</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.319235</td>\n",
              "      <td>-0.890480</td>\n",
              "      <td>-1.616363</td>\n",
              "      <td>-0.270188</td>\n",
              "      <td>-0.899197</td>\n",
              "      <td>-1.257106</td>\n",
              "      <td>0.293388</td>\n",
              "      <td>0.590899</td>\n",
              "      <td>0.221342</td>\n",
              "      <td>3.205504</td>\n",
              "      <td>1.10808</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879364</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.091491</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076972</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010336</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.21498</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.554907</td>\n",
              "      <td>-0.148836</td>\n",
              "      <td>-0.681563</td>\n",
              "      <td>-0.006719</td>\n",
              "      <td>0.318742</td>\n",
              "      <td>0.532212</td>\n",
              "      <td>0.816364</td>\n",
              "      <td>0.742654</td>\n",
              "      <td>-0.196691</td>\n",
              "      <td>3.126965</td>\n",
              "      <td>1.10808</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879364</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.091491</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>4.287867</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076972</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.21498</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.622768</td>\n",
              "      <td>-0.005869</td>\n",
              "      <td>0.520322</td>\n",
              "      <td>-0.129044</td>\n",
              "      <td>1.227908</td>\n",
              "      <td>0.474492</td>\n",
              "      <td>0.965786</td>\n",
              "      <td>0.742654</td>\n",
              "      <td>-0.536343</td>\n",
              "      <td>3.194931</td>\n",
              "      <td>1.10808</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879364</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.091491</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076972</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>4.272931</td>\n",
              "      <td>-0.21498</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.301377</td>\n",
              "      <td>-0.988770</td>\n",
              "      <td>-1.616363</td>\n",
              "      <td>-0.547771</td>\n",
              "      <td>-0.813427</td>\n",
              "      <td>-1.256464</td>\n",
              "      <td>0.293388</td>\n",
              "      <td>0.540313</td>\n",
              "      <td>0.195215</td>\n",
              "      <td>3.165479</td>\n",
              "      <td>1.10808</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879364</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.091491</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076972</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010336</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.21498</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        51        52        53\n",
              "0 -1.297805 -0.935157 -1.482820  ... -0.165956 -0.156014 -0.123654\n",
              "1 -1.319235 -0.890480 -1.616363  ... -0.165956 -0.156014 -0.123654\n",
              "2 -0.554907 -0.148836 -0.681563  ... -0.165956 -0.156014 -0.123654\n",
              "3 -0.622768 -0.005869  0.520322  ... -0.165956 -0.156014 -0.123654\n",
              "4 -1.301377 -0.988770 -1.616363  ... -0.165956 -0.156014 -0.123654\n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RhAdjgMp3t"
      },
      "source": [
        "##### 3) Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "h45M5N6IMu87",
        "outputId": "91d42bc4-5955-4f88-9393-bbc803f84505"
      },
      "source": [
        "normalizer = preprocessing.Normalizer().fit(covtype_X)\n",
        "covtype_X_normalized = pd.DataFrame(normalizer.transform(covtype_X))\n",
        "covtype_X_normalized.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.380210</td>\n",
              "      <td>0.007469</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.037787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.074695</td>\n",
              "      <td>0.032368</td>\n",
              "      <td>0.033979</td>\n",
              "      <td>0.021676</td>\n",
              "      <td>0.919622</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.382769</td>\n",
              "      <td>0.008276</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.031331</td>\n",
              "      <td>-0.000887</td>\n",
              "      <td>0.057637</td>\n",
              "      <td>0.032513</td>\n",
              "      <td>0.034730</td>\n",
              "      <td>0.022316</td>\n",
              "      <td>0.919975</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.375821</td>\n",
              "      <td>0.018630</td>\n",
              "      <td>0.001206</td>\n",
              "      <td>0.035920</td>\n",
              "      <td>0.008712</td>\n",
              "      <td>0.426216</td>\n",
              "      <td>0.031363</td>\n",
              "      <td>0.031899</td>\n",
              "      <td>0.018094</td>\n",
              "      <td>0.820399</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.371810</td>\n",
              "      <td>0.020693</td>\n",
              "      <td>0.002403</td>\n",
              "      <td>0.032308</td>\n",
              "      <td>0.015754</td>\n",
              "      <td>0.412529</td>\n",
              "      <td>0.031774</td>\n",
              "      <td>0.031774</td>\n",
              "      <td>0.016288</td>\n",
              "      <td>0.829196</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.386275</td>\n",
              "      <td>0.006698</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.022775</td>\n",
              "      <td>-0.000149</td>\n",
              "      <td>0.058202</td>\n",
              "      <td>0.032748</td>\n",
              "      <td>0.034832</td>\n",
              "      <td>0.022328</td>\n",
              "      <td>0.918725</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4   ...   49   50   51   52   53\n",
              "0  0.380210  0.007469  0.000439  0.037787  0.000000  ...  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.382769  0.008276  0.000296  0.031331 -0.000887  ...  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.375821  0.018630  0.001206  0.035920  0.008712  ...  0.0  0.0  0.0  0.0  0.0\n",
              "3  0.371810  0.020693  0.002403  0.032308  0.015754  ...  0.0  0.0  0.0  0.0  0.0\n",
              "4  0.386275  0.006698  0.000298  0.022775 -0.000149  ...  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwWevUryCDF6"
      },
      "source": [
        "Preprocessing target variable, keeping 2 as positive class and for others assigning value of 0 for representing negative class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZlJF7QmCUTk",
        "outputId": "a5373e98-0477-490e-fd74-cbf6ae5ff2af"
      },
      "source": [
        "covtype_Y = covtype_Y.apply(lambda x: 2 if (x == 2)  else 0)\n",
        "covtype_Y.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    2\n",
              "3    2\n",
              "4    0\n",
              "Name: 54, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71TkERZUBWCU"
      },
      "source": [
        "Converting data to libsvm format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaJ55wEcBdNt"
      },
      "source": [
        "from sklearn.datasets import dump_svmlight_file\n",
        "\n",
        "### Dumping Raw Data ###\n",
        "dump_svmlight_file(covtype_X.values, list(covtype_Y),'./sample_data/covtype_libsvm.data', zero_based=True, multilabel=False)\n",
        "### Dumping Rescaled Data ###\n",
        "dump_svmlight_file(covtype_X_rescaled.values,list(covtype_Y),'./sample_data/covtype_rescaled_libsvm.data', zero_based=True, multilabel=False)\n",
        "### Dumping Standardized Data ###\n",
        "dump_svmlight_file(covtype_X_standardized.values,list(covtype_Y),'./sample_data/covtype_standardized_libsvm.data', zero_based=True, multilabel=False)\n",
        "### Dumping Normalized Data ###\n",
        "dump_svmlight_file(covtype_X_normalized.values,list(covtype_Y),'./sample_data/covtype_normalized_libsvm.data', zero_based=True, multilabel=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px6k_aqqE7VE"
      },
      "source": [
        "Reading data in libsvm format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytllGsh_DNW2"
      },
      "source": [
        " ### Reading Raw Data ###\n",
        " y_covtype, x_covtype = svm_read_problem('./sample_data/covtype_libsvm.data')\n",
        " ### Reading Rescaled Data ###\n",
        " y_covtype_rescaled, x_covtype_rescaled = svm_read_problem('./sample_data/covtype_rescaled_libsvm.data')\n",
        "### Reading Standardized Data ###\n",
        " y_covtype_standardized, x_covtype_standardized = svm_read_problem('./sample_data/covtype_standardized_libsvm.data')\n",
        "### Reading Normalized Data ###\n",
        " y_covtype_normalized, x_covtype_normalized = svm_read_problem('./sample_data/covtype_normalized_libsvm.data')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve3xd6sJW1Wk"
      },
      "source": [
        "Reading training and testing indexes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiPMgEQW1Wv"
      },
      "source": [
        "with open('./sample_data/covtype.train.index.txt') as f:    \n",
        "    covtype_train_indexes = [int(line) for line in f]\n",
        "with open('./sample_data/covtype.test.index.txt') as f:    \n",
        "    covtype_test_indexes = [int(line) for line in f]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6StbGEawXqmr"
      },
      "source": [
        "Splitting training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW2HeUuaXqmr"
      },
      "source": [
        "### Splitting Raw Data ###\n",
        "x_covtype_train = [x_covtype[i - 1] for i in covtype_train_indexes]\n",
        "y_covtype_train = [y_covtype[i - 1] for i in covtype_train_indexes]\n",
        "x_covtype_test = [x_covtype[i - 1] for i in covtype_test_indexes]\n",
        "y_covtype_test = [y_covtype[i - 1] for i in covtype_test_indexes]\n",
        "\n",
        "### Splitting Rescaled Data ###\n",
        "x_covtype_rescaled_train = [x_covtype_rescaled[i - 1] for i in covtype_train_indexes]\n",
        "y_covtype_rescaled_train = [y_covtype_rescaled[i - 1] for i in covtype_train_indexes]\n",
        "x_covtype_rescaled_test = [x_covtype_rescaled[i - 1] for i in covtype_test_indexes]\n",
        "y_covtype_rescaled_test = [y_covtype_rescaled[i - 1] for i in covtype_test_indexes]\n",
        "\n",
        "### Splitting Standardized Data ###\n",
        "x_covtype_standardized_train = [x_covtype_standardized[i - 1] for i in covtype_train_indexes]\n",
        "y_covtype_standardized_train = [y_covtype_standardized[i - 1] for i in covtype_train_indexes]\n",
        "x_covtype_standardized_test = [x_covtype_standardized[i - 1] for i in covtype_test_indexes]\n",
        "y_covtype_standardized_test = [y_covtype_standardized[i - 1] for i in covtype_test_indexes]\n",
        "\n",
        "### Splitting Normalized Data ###\n",
        "x_covtype_normalized_train = [x_covtype_normalized[i - 1] for i in covtype_train_indexes]\n",
        "y_covtype_normalized_train = [y_covtype_normalized[i - 1] for i in covtype_train_indexes]\n",
        "x_covtype_normalized_test = [x_covtype_normalized[i - 1] for i in covtype_test_indexes]\n",
        "y_covtype_normalized_test = [y_covtype_normalized[i - 1] for i in covtype_test_indexes]\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6sazO4xZnn4"
      },
      "source": [
        "##### Training and Evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88F6023vat1z"
      },
      "source": [
        "Defining utility functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6eFDc5NZzQx"
      },
      "source": [
        "# function to get best value of C using 5-fold cross validation\n",
        "def getBestC(x_train, y_train):\n",
        "  candidate_C = ['0.1', '1', '10', '100', '1000']\n",
        "  min_val_error = 100\n",
        "  min_val_error_C = candidate_C[0]\n",
        "  for C in candidate_C:\n",
        "    print(\"For C = \" + C)\n",
        "    parameters = '-s 3 -v 5 -c ' + C\n",
        "    CV_ACC = train(y_train, x_train, parameters)\n",
        "    val_error = 100 - CV_ACC\n",
        "    print(\"Validation Error: \" + str(val_error))\n",
        "    p_label, p_acc, p_val = predict(y_train, x_train, train(y_train, x_train, '-s 3 -c ' + C))\n",
        "    print(\"Training Error: \" + str(100 - p_acc[0]))\n",
        "\n",
        "    if val_error < min_val_error:\n",
        "      min_val_error = val_error\n",
        "      min_val_error_C = C\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "  print(\"Best C = \" + min_val_error_C)\n",
        "\n",
        "  return min_val_error_C\n",
        "\n",
        "def trainModel(x_train, y_train, C):\n",
        "  # training model\n",
        "  prob  = problem(y_train, x_train)\n",
        "  param = parameter('-s 3 -c ' + C)\n",
        "  m = train(prob, param)\n",
        "\n",
        "  return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mAveOoQbBAv"
      },
      "source": [
        "Training and Evaluating Rescaled Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxeJLQPPbGc6"
      },
      "source": [
        "print(\"--- Cross Validation ---\")\n",
        "bestC_rescaled = getBestC(x_covtype_rescaled_train, y_covtype_rescaled_train)\n",
        "model_rescaled = trainModel(x_covtype_rescaled_train, y_covtype_rescaled_train, bestC_rescaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLvDruS4fKHK"
      },
      "source": [
        "# predicting\n",
        "print(\"----- Evaluation -----\")\n",
        "p_label_rescaled, p_acc_rescaled, p_val_rescaled = predict(y_covtype_rescaled_test, x_covtype_rescaled_test, model_rescaled)\n",
        "ACC_rescaled, MSE_rescaled, SCC_rescaled = evaluations(y_covtype_rescaled_test, p_label_rescaled)\n",
        "rescaled_error_rate = 100 - ACC_rescaled\n",
        "print(\"Test Error Rate = \" + str(rescaled_error_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZYvTCAwAqye"
      },
      "source": [
        "# saving objects for future use (helpful in case of interruptions)\n",
        "import pickle\n",
        "filename = 'p_label_rescaled.sav'\n",
        "pickle.dump(p_label_rescaled, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdTbIGYor5SY"
      },
      "source": [
        "Training and Evaluating Standardized Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txks55lLsAy7"
      },
      "source": [
        "print(\"--- Cross Validation and Model Training ---\")\n",
        "bestC_standardized = getBestC(x_covtype_standardized_train, y_covtype_standardized_train)\n",
        "model_standardized = trainModel(x_covtype_standardized_train, y_covtype_standardized_train, bestC_standardized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j52iuXPKuoBG"
      },
      "source": [
        "# predicting\n",
        "print(\"----- Evaluation -----\")\n",
        "p_label_standardized, p_acc_standardized, p_val_standardized = predict(y_covtype_standardized_test, x_covtype_standardized_test, model_standardized)\n",
        "ACC_standardized, MSE_standardized, SCC_standardized = evaluations(y_covtype_standardized_test, p_label_standardized)\n",
        "standardized_error_rate = 100 - ACC_rescaled\n",
        "print(\"Test Error Rate = \" + str(standardized_error_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCjjEwOzGavE"
      },
      "source": [
        "# saving objects for future use (helpful in case of interruptions)\n",
        "import pickle\n",
        "filename = 'p_label_standardized.sav'\n",
        "pickle.dump(p_label_standardized, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqIJTeBL5Mbf"
      },
      "source": [
        "Training and Evaluating Normalized Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWqFUnnQ5QK5",
        "outputId": "1d35c427-d12c-414c-8cd3-4a3c53c81f24"
      },
      "source": [
        "print(\"--- Cross Validation Model Training ---\")\n",
        "bestC_normalized = getBestC(x_covtype_normalized_train, y_covtype_normalized_train)\n",
        "model_normalized = trainModel(x_covtype_normalized_train, y_covtype_normalized_train, bestC_normalized)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Cross Validation Model Training ---\n",
            "For C = 0.1\n",
            "Cross Validation Accuracy = 50.7046%\n",
            "Validation Error: 49.29538678666159\n",
            "Accuracy = 51.2202% (267836/522911) (classification)\n",
            "Training Error: 48.77981147843514\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 50.6767%\n",
            "Validation Error: 49.32330740795279\n",
            "Accuracy = 51.2202% (267836/522911) (classification)\n",
            "Training Error: 48.77981147843514\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 50.6507%\n",
            "Validation Error: 49.349315657922666\n",
            "Accuracy = 51.2202% (267836/522911) (classification)\n",
            "Training Error: 48.77981147843514\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 50.7218%\n",
            "Validation Error: 49.27817544476976\n",
            "Accuracy = 51.2202% (267836/522911) (classification)\n",
            "Training Error: 48.77981147843514\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 48.7798%\n",
            "Validation Error: 51.22018852156486\n",
            "Accuracy = 51.2202% (267836/522911) (classification)\n",
            "Training Error: 48.77981147843514\n",
            "\n",
            "Best C = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vru1s6ri5eA_",
        "outputId": "4e8efe2d-210f-45fa-88c2-f5187be34ae1"
      },
      "source": [
        "# predicting\n",
        "print(\"----- Evaluation -----\")\n",
        "p_label_normalized, p_acc_normalized, p_val_normalized = predict(y_covtype_normalized_test, x_covtype_normalized_test, model_normalized)\n",
        "ACC_normalized, MSE_normalized, SCC_normalized = evaluations(y_covtype_normalized_test, p_label_normalized)\n",
        "normalized_error_rate = 100 - ACC_normalized\n",
        "print(\"Test Error Rate = \" + str(normalized_error_rate))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Evaluation -----\n",
            "Accuracy = 51.4191% (29875/58101) (classification)\n",
            "Test Error Rate = 48.58091943340046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Z4hEnLHx8c"
      },
      "source": [
        "# saving objects for future use (helpful in case of interruptions)\n",
        "import pickle\n",
        "filename = 'p_label_normalized.sav'\n",
        "pickle.dump(p_label_normalized, open(filename, 'wb'))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7yD2qO9Igkh"
      },
      "source": [
        "Training and Evaluating Raw Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_2P4jdIIgkp",
        "outputId": "02610a46-ba9d-490d-e30e-bce4f4e8a80c"
      },
      "source": [
        "print(\"--- Cross Validation Model Training ---\")\n",
        "bestC = getBestC(x_covtype_train, y_covtype_train)\n",
        "model = trainModel(x_covtype_train, y_covtype_train, bestC)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Cross Validation Model Training ---\n",
            "For C = 0.1\n",
            "Cross Validation Accuracy = 54.0526%\n",
            "Validation Error: 45.947398314435915\n",
            "Accuracy = 62.9034% (328929/522911) (classification)\n",
            "Training Error: 37.096561365127144\n",
            "\n",
            "For C = 1\n",
            "Cross Validation Accuracy = 55.0788%\n",
            "Validation Error: 44.921219863418436\n",
            "Accuracy = 49.2841% (257712/522911) (classification)\n",
            "Training Error: 50.71589620413416\n",
            "\n",
            "For C = 10\n",
            "Cross Validation Accuracy = 53.5466%\n",
            "Validation Error: 46.453411766055794\n",
            "Accuracy = 60.866% (318275/522911) (classification)\n",
            "Training Error: 39.13400177085584\n",
            "\n",
            "For C = 100\n",
            "Cross Validation Accuracy = 55.9489%\n",
            "Validation Error: 44.051090912220246\n",
            "Accuracy = 52.2915% (273438/522911) (classification)\n",
            "Training Error: 47.708501064234646\n",
            "\n",
            "For C = 1000\n",
            "Cross Validation Accuracy = 54.0082%\n",
            "Validation Error: 45.99176532909042\n",
            "Accuracy = 59.0556% (308808/522911) (classification)\n",
            "Training Error: 40.94444370074448\n",
            "\n",
            "Best C = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxmYarAvIgkq",
        "outputId": "0e95fd96-8e04-412c-ac28-055e839d44e4"
      },
      "source": [
        "# predicting\n",
        "print(\"----- Evaluation -----\")\n",
        "p_label, p_acc, p_val = predict(y_covtype_test, x_covtype_test, model)\n",
        "ACC, MSE, SCC = evaluations(y_covtype_test, p_label)\n",
        "error_rate = 100 - ACC\n",
        "print(\"Test Error Rate = \" + str(error_rate))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Evaluation -----\n",
            "Accuracy = 51.4225% (29877/58101) (classification)\n",
            "Test Error Rate = 48.57747715185625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agAM6BAGIgkr"
      },
      "source": [
        "# saving objects for future use (helpful in case of interruptions)\n",
        "import pickle\n",
        "filename = 'p_label.sav'\n",
        "pickle.dump(p_label, open(filename, 'wb'))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExwYYBavmbSZ"
      },
      "source": [
        "import pickle\n",
        "p_label = pickle.load(open('./sample_data/p_label.sav', 'rb'))\n",
        "p_label_normalized = pickle.load(open('./sample_data/p_label_normalized.sav', 'rb'))\n",
        "p_label_rescaled = pickle.load(open('./sample_data/p_label_rescaled.sav', 'rb'))\n",
        "p_label_standardized = pickle.load(open('./sample_data/p_label_standardized.sav', 'rb'))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlsMcTvJnEYc"
      },
      "source": [
        "#### Part - 2: Calculating Accuracy, F-1 Score, AUC on the testing data using each preprocessed data and raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyi3I1A4nRcX"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU-aL2mTqToE"
      },
      "source": [
        "Calculating metrics for raw data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXkPi67brBZK",
        "outputId": "bc56df93-534f-490a-cc42-0768c3af6b72"
      },
      "source": [
        "accuracy_raw = accuracy_score(y_covtype_test, p_label)\n",
        "\n",
        "f1_score_raw = f1_score(y_covtype_test, p_label, pos_label=2.0)\n",
        "\n",
        "auc_raw = roc_auc_score(y_covtype_test, p_label)\n",
        "\n",
        "print(\"--- Raw Data Metrics ---\")\n",
        "print(\"Accuracy: \" + str(accuracy_raw))\n",
        "print(\"F1-Score: \" + str(f1_score_raw))\n",
        "print(\"AUC: \" + str(auc_raw))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Raw Data Metrics ---\n",
            "Accuracy: 0.5142252284814375\n",
            "F1-Score: 0.00014170327334561429\n",
            "AUC: 0.5000354283284915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCBQ6SDL35gw"
      },
      "source": [
        "Calculating Metrics for Rescaled Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8YD0m4n38XW",
        "outputId": "367a935d-d488-4acb-f095-504cb484dbcf"
      },
      "source": [
        "accuracy_rescaled = accuracy_score(y_covtype_test, p_label_rescaled)\n",
        "\n",
        "f1_score_rescaled = f1_score(y_covtype_test, p_label_rescaled, pos_label=2.0)\n",
        "\n",
        "auc_rescaled = roc_auc_score(y_covtype_test, p_label_rescaled)\n",
        "\n",
        "print(\"--- Rescaled Data Metrics ---\")\n",
        "print(\"Accuracy: \" + str(accuracy_rescaled))\n",
        "print(\"F1-Score: \" + str(f1_score_rescaled))\n",
        "print(\"AUC: \" + str(auc_rescaled))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Rescaled Data Metrics ---\n",
            "Accuracy: 0.5141908056659954\n",
            "F1-Score: 0.0\n",
            "AUC: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm3eApqp4NLt"
      },
      "source": [
        "Calculating Metrics for Standardized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lqtWSpt4TNS",
        "outputId": "e1c1cb35-69d6-4cb0-b3b7-e0e01c07d51b"
      },
      "source": [
        "accuracy_standardized = accuracy_score(y_covtype_test, p_label_standardized)\n",
        "\n",
        "f1_score_standardized = f1_score(y_covtype_test, p_label_standardized, pos_label=2.0)\n",
        "\n",
        "auc_standardized = roc_auc_score(y_covtype_test, p_label_standardized)\n",
        "\n",
        "print(\"--- Standardized Data Metrics ---\")\n",
        "print(\"Accuracy: \" + str(accuracy_standardized))\n",
        "print(\"F1-Score: \" + str(f1_score_standardized))\n",
        "print(\"AUC: \" + str(auc_standardized))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Standardized Data Metrics ---\n",
            "Accuracy: 0.6267017779384176\n",
            "F1-Score: 0.5948783084595701\n",
            "AUC: 0.6249757489385586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygI-QHw35Sr8"
      },
      "source": [
        "Calculating Metrics of Normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwKL7bFy5WnG",
        "outputId": "7362c8b4-93e5-471b-b8ea-865761fb487a"
      },
      "source": [
        "accuracy_normalized = accuracy_score(y_covtype_test, p_label_normalized)\n",
        "\n",
        "f1_score_normalized = f1_score(y_covtype_test, p_label_normalized, pos_label=2.0)\n",
        "\n",
        "auc_normalized = roc_auc_score(y_covtype_test, p_label_normalized)\n",
        "\n",
        "print(\"--- Normalized Data Metrics ---\")\n",
        "print(\"Accuracy: \" + str(accuracy_normalized))\n",
        "print(\"F1-Score: \" + str(f1_score_normalized))\n",
        "print(\"AUC: \" + str(auc_normalized))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Normalized Data Metrics ---\n",
            "Accuracy: 0.5141908056659954\n",
            "F1-Score: 0.0\n",
            "AUC: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsYa5g6d5-8q"
      },
      "source": [
        "Creating metrics table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-obUS686Bh5",
        "outputId": "3e9f1abd-d576-4355-b89a-e227fac55b8d"
      },
      "source": [
        "metrics = {'Data':  ['Raw', 'Rescaled','Standardized', 'Normalized'],\n",
        "          'Accuracy':  [accuracy_raw, accuracy_rescaled, accuracy_standardized,accuracy_normalized],\n",
        "          'F1-Score': [f1_score_raw, f1_score_rescaled, f1_score_standardized, f1_score_normalized],\n",
        "          'AUC': [auc_raw,auc_rescaled,auc_standardized, auc_normalized]}\n",
        "\n",
        "df = pd.DataFrame (metrics, columns = ['Data', 'Accuracy','F1-Score','AUC'])\n",
        "\n",
        "print (df)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Data  Accuracy  F1-Score       AUC\n",
            "0           Raw  0.514225  0.000142  0.500035\n",
            "1      Rescaled  0.514191  0.000000  0.500000\n",
            "2  Standardized  0.626702  0.594878  0.624976\n",
            "3    Normalized  0.514191  0.000000  0.500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uikTVxrC7ooI"
      },
      "source": [
        "#### Part - 3: Plotting ROC curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6QWlcez7ssW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvw7PbmcCaw8"
      },
      "source": [
        "ROC curve for raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "KvLR4O_a8eYb",
        "outputId": "b8674012-ef3c-4863-bd2a-1b36016a4e47"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_covtype_test, p_label, pos_label=2.0)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\", auc=\"+str(auc_raw))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZGne8UCehv"
      },
      "source": [
        "ROC curve for rescaled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "3A9aCWif-E-b",
        "outputId": "b4a070aa-8969-48ef-8f59-5b86f0be456a"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_covtype_test, p_label_rescaled, pos_label=2.0)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\", auc=\"+str(auc_rescaled))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlKEDA3zCh4X"
      },
      "source": [
        "ROC curve for standardized data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "4ZemTK1b-UDU",
        "outputId": "71d12170-c460-4e7b-977d-287dbc4ff73a"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_covtype_test, p_label_standardized, pos_label=2.0)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\", auc=\"+str(auc_standardized))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfBOc_6RCl9Y"
      },
      "source": [
        "ROC curve normalized data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "16pGwk7I-Z89",
        "outputId": "8367b447-2a6a-4e7f-abae-dcda2ffd7a02"
      },
      "source": [
        "fpr, tpr, _ = roc_curve(y_covtype_test, p_label_normalized, pos_label=2.0)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\", auc=\"+str(auc_normalized))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km4zOESSCpNV"
      },
      "source": [
        "#### Part - 4: Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6eMscvqCtep"
      },
      "source": [
        "It is clear from above results that standardized data preprocessing technique yields better metrics (accuracy, f1-score auc) than other preprocessing techniques and raw data. There could be one or two possible reasons for this result:\n",
        "\n",
        "1) Dataset follows a Gaussian Distribution.\n",
        "\n",
        "OR\n",
        "\n",
        "2) There are too many outlier values in the dataset which is affecting performance for other dataset but standardized dataset remains unaffected by the presence of outliers."
      ]
    }
  ]
}